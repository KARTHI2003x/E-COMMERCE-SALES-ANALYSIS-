{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "TS-1a: smoothing methods",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'tsdata-1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1342431%2F2592705%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240416%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240416T103929Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db8b6bd66d6fbbccb9a23f4cf18d4212b6d7358d00319175af7b8ac849ad7714d5dbca25cedb3e6bebe472285a03ee5bf3eed63ec62f3924b0977eb393d89d86aa2b56518cfbee63e55eedde6b5035cf4b3f8030d13344cb95580446e9f5eec70545796590debc26378bc9e9b7ee3ea7c773466fb450f2fc24a7da0bad65b228bdc38d97af9a8bb15103efb185f125157b18db5b0d483d38b0985166aa9a50807b3ea9d703d3f6e779e859b4f5fc94600a4892e36f58301e2ecc160f9eaa4435102cc736592b366a4d91b0de584ba2d0ce163789da9f3034e96415ee870c7d1a9ed5dcfc2eb3b0f0bd06de70244392f8473ad46b1eb733a541fcda31b7946c7d1'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "WjDYNOetfZH8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is part of a series of notebooks about practical time series methods:\n",
        "\n",
        "* [Part 0: the basics](https://www.kaggle.com/konradb/ts-0-the-basics)\n",
        "* [Part 1a: smoothing methods](https://www.kaggle.com/konradb/ts-1a-smoothing-methods) - **this notebook**\n",
        "* [Part 1b: Prophet](https://www.kaggle.com/konradb/ts-1b-prophet)\n",
        "* [Part 2: ARMA](https://www.kaggle.com/konradb/ts-2-arma-and-friends)\n",
        "* [Part 3: Time series for finance](https://www.kaggle.com/konradb/ts-3-time-series-for-finance)\n",
        "* [Part 4: Sales and demand forecasting](https://www.kaggle.com/konradb/ts-4-sales-and-demand-forecasting)\n",
        "* [Part 5: Automatic for the people](https://www.kaggle.com/code/konradb/ts-5-automatic-for-the-people)\n",
        "* [Part 6: Deep learning for TS - sequences](https://www.kaggle.com/konradb/ts-6-deep-learning-for-ts-sequences)\n",
        "* [Part 7: Survival analysis](https://www.kaggle.com/konradb/ts-7-survival-analysis)\n",
        "* [Part 8: Hierarchical time series](https://www.kaggle.com/code/konradb/ts-8-hierarchical-time-series)\n",
        "* [Part 9: Hybrid methods](https://www.kaggle.com/code/konradb/ts-9-hybrid-methods/)\n",
        "* [Part 10: Validation methods for time series](https://www.kaggle.com/code/konradb/ts-10-validation-methods-for-time-series/)\n",
        "* [Part 11: Transfer learning](https://www.kaggle.com/code/konradb/ts-11-deep-learning-for-ts-transfer-learning)\n",
        "\n",
        "\n",
        "The series is accompanied by video presentations on the YouTube channel of [Abhishek](https://www.kaggle.com/abhishek):\n",
        "\n",
        "* [Talk 0](https://www.youtube.com/watch?v=cKzXOOtOXYY)\n",
        "* [Talk 1](https://www.youtube.com/watch?v=kAI67Sz92-s) - combining the content from parts 1a and 1b\n",
        "* [Talk 2](https://www.youtube.com/watch?v=LjV5DE3KR-U)\n",
        "* [Talk 3](https://www.youtube.com/watch?v=74rDhJexmTg)\n",
        "* [Talk 4](https://www.youtube.com/watch?v=RdH8zd07u2E)\n",
        "* [Talk 5](https://www.youtube.com/watch?v=wBP8Pc4Wxzs)\n",
        "* [Talk 6](https://www.youtube.com/watch?v=81AEI0tj0Kk)\n",
        "* [Talk 7](https://www.youtube.com/watch?v=m-8I_hkmz9o)\n",
        "* [Talk 8](https://www.youtube.com/watch?v=7ZTarg4QYR4)\n",
        "* [Talk 9](https://www.youtube.com/watch?v=NYZzBvKcfp4)\n",
        "* [Talk 10](https://www.youtube.com/watch?v=47WeBiLV2Uo)\n",
        "* [Talk 11]()\n",
        "\n",
        "\n",
        "**If you think this notebook deserves an upvote, I'd love to have it. An upvote per view, its all I ask**\n",
        "(credit to [Dan Carlin](https://twitter.com/HardcoreHistory) for coining the phrase ;-)\n",
        "\n",
        "\n",
        "---------------------------------------\n",
        "\n",
        "\n",
        "In this instalment, we cover vintage time series prediction methods introduced at the dawn of modern statistics era, i.e. in the 1950s by - among other - Brown, Holt and Winters. Their ideas were variations around the topic of exponential smoothing: weighted averages of past observations, with the weights decaying exponentially as the observations get older. A huge practical advantage of those methods that one only needs the previous smoothed value and current time series value to create a forecast for the next step - an important aspect in an era when a computer had less memory than a cheapest smartphone available in the market today.\n",
        "\n",
        "As computing power became more available, exponential smoothing methods started fading into the \"historical\" section of statistics guide - until people started realizing that with the latency requirements of trading or e-commerce, speed is more important that additional 5pct improvement in error and one-step forecasts started making a comeback.\n",
        "\n",
        "Below, I present the most popular exponential smoothing methods - along with some practical tips and suggestions.\n",
        "\n",
        "* [Exponential smoothing ](#section-one)\n",
        "* [Popular methods](#section-two)\n",
        "* [Anomaly detection](#section-three)\n",
        "* [Example pipeline](#section-four)"
      ],
      "metadata": {
        "id": "TUYb9sWmfZIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from random import gauss\n",
        "from pandas.plotting import autocorrelation_plot\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from random import random\n",
        "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing, Holt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category= FutureWarning)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:11.826943Z",
          "iopub.execute_input": "2021-07-15T22:38:11.827722Z",
          "iopub.status.idle": "2021-07-15T22:38:12.504575Z",
          "shell.execute_reply.started": "2021-07-15T22:38:11.827562Z",
          "shell.execute_reply": "2021-07-15T22:38:12.503501Z"
        },
        "trusted": true,
        "id": "9MmRcHvUfZIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# general settings\n",
        "class CFG:\n",
        "    data_folder = '../input/tsdata-1/'\n",
        "    img_dim1 = 20\n",
        "    img_dim2 = 10\n",
        "\n",
        "# adjust the parameters for displayed figures\n",
        "plt.rcParams.update({'figure.figsize': (CFG.img_dim1,CFG.img_dim2)})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:12.506198Z",
          "iopub.execute_input": "2021-07-15T22:38:12.506558Z",
          "iopub.status.idle": "2021-07-15T22:38:12.513249Z",
          "shell.execute_reply.started": "2021-07-15T22:38:12.506521Z",
          "shell.execute_reply": "2021-07-15T22:38:12.512157Z"
        },
        "trusted": true,
        "id": "Yel8QFKkfZIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-one\"></a>\n",
        "# Exponential smoothing"
      ],
      "metadata": {
        "id": "RgTRxRldfZIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned above, exponential moving average (EMA) assigns exponentially decreasing weights over time. The functions as a low-pass filter that removes high-frequency noise (and can be formulated as a special case of a more general problem of recursive filtering). Exponential smoothing models discussed in this module can be formulated as a special case of a general framework of state space models - those are discussed in part 4 of this series (there will be a link to the notebook once I've written it ;-)\n",
        "\n",
        "A complete taxonomy of ETS (error, trend, seasonality) models can be found in Hyndman et al \"Forecasting with Exponential Smoothing: The State Space Approach\" - below we discuss the three most popular models from that class.\n",
        "\n",
        "<a id=\"section-two\"></a>\n",
        "# Popular methods\n",
        "\n",
        "## EWMA\n",
        "\n",
        "Simple Exponential Smoothing (Brown method) is defined by the relationship:\n",
        "\n",
        "\\begin{equation}\n",
        "S_t = \\alpha X_t + (1-\\alpha) S_{t-1} \\quad \\text{where} \\quad \\alpha \\in (0,1)\n",
        "\\end{equation}\n",
        "\n",
        "or equivalently:\n",
        "\n",
        "\\begin{equation}\n",
        "S_t = S_{t-1} + \\alpha  (X_t - S_{t-1})\n",
        "\\end{equation}\n",
        "\n",
        "Few observations around that definition:\n",
        "* the smoothed series is a simple weighted average of the past and the present\n",
        "* interpretation of smoothing factor $\\alpha$: recency vs smoothing (see below). It defines how quickly we will \"forget\" the last available true observation.\n",
        "* $\\alpha$ is selected on the basis of expert judgement or estimated (with MSE); statsmodels does the estimation by default\n",
        "* by its very nature, smoothing needs some time to catch up with the dynamics of your time series. A rule of thumb for a reasonable sample size is that you need $\\frac{3}{\\alpha}$ observations.\n",
        "* Exponentiality is hidden in the recursiveness of the function -- we multiply by  $(1−\\alpha)$  each time, which already contains a multiplication by the same factor of previous model values.\n",
        "* the method is suitable for forecasting data with no clear trend or seasonal pattern\n",
        "\n",
        "With the setup of the above equation, we have the following form of a long term forecast:\n",
        "\\begin{equation}\n",
        "\\hat{X}_{t+h} = S_t\n",
        "\\end{equation}\n",
        "\n",
        "which means simply that out of sample, our forecast is equal to the most recent value of the smoothed series.\n",
        "\n",
        "It's an old cliche that a picture is worth a thousand words - so the three pictures below should give you a truly excellent intuition ;-) on how single exponential smoothing works.\n",
        "\n"
      ],
      "metadata": {
        "id": "lGTo2tY4fZIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha_sm in [0.2 , 0.5, 0.9]:\n",
        "\n",
        "    df = pd.read_csv(CFG.data_folder + 'exp1.csv', header = None); df.columns = ['series']\n",
        "    df.plot.line()\n",
        "    fit1 = SimpleExpSmoothing(df).fit(smoothing_level = alpha_sm  ,optimized=False)\n",
        "    fcast1 = fit1.forecast(12).rename('alpha = ' + str(alpha_sm))\n",
        "    fcast1.plot(marker='o', color='red', legend=True)\n",
        "    fit1.fittedvalues.plot(  color='red')\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:12.516289Z",
          "iopub.execute_input": "2021-07-15T22:38:12.516755Z",
          "iopub.status.idle": "2021-07-15T22:38:13.694317Z",
          "shell.execute_reply.started": "2021-07-15T22:38:12.516718Z",
          "shell.execute_reply": "2021-07-15T22:38:13.693303Z"
        },
        "trusted": true,
        "id": "-XLdVAz4fZIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the graphs above, the for small values of the smoothing constant $\\alpha$ most of the variation has been removed and we have a series following just the general trend; on the other hand, high value of the $\\alpha$ parameter results in hardly any smoothing at all and the new series follows the original very closely (albeit with a delay, which is obvious given the relationship between raw and smoothed values).\n",
        "\n",
        "\n",
        "**Pro tip**: anytime you are using exponential smoothing that you did not write yourself, double-check the parametrization - does small $\\alpha$ mean heavy smoothing or hardly any at all? The idea that the coefficient closer to 1 means less smoothing is merely a convention.\n",
        "\n",
        "\n",
        "What happens if we apply the method to the passengers dataset, first introduced in [part 1](https://www.kaggle.com/konradb/practical-time-series-part-1-the-basics) ?"
      ],
      "metadata": {
        "id": "uDOsDyAUfZIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.5\n",
        "\n",
        "df = pd.read_csv(CFG.data_folder + 'passengers.csv', usecols = ['passengers'])\n",
        "df.plot.line()\n",
        "fit1 = SimpleExpSmoothing(df).fit(smoothing_level= alpha,optimized=False)\n",
        "fcast1 = fit1.forecast(12).rename(r'$\\alpha=0.5$')\n",
        "fcast1.plot(marker='o', color='red', legend=True)\n",
        "fit1.fittedvalues.plot(  color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:13.695955Z",
          "iopub.execute_input": "2021-07-15T22:38:13.696249Z",
          "iopub.status.idle": "2021-07-15T22:38:14.14189Z",
          "shell.execute_reply.started": "2021-07-15T22:38:13.696222Z",
          "shell.execute_reply": "2021-07-15T22:38:14.140606Z"
        },
        "trusted": true,
        "id": "U8nDmRCSfZIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the one hand, the model does exactly what we expect it to do: beyond the range of the original data, it propagates the most recent smoothed value. On the other hand - it is not, to put it mildly, realistic to expect the number of passengers to flatline and the trend to disappear."
      ],
      "metadata": {
        "id": "C0zM6LOufZIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Double Exponential Smoothing\n",
        "\n",
        "Moving towards double exponential smoothing is akin to taking one more component in the seasonal decomposition: we began with level only, and we take into account the trend as well. We have learnt to predict intercept with our previous methods; now, we will apply the same exponential smoothing to the trend by assuming that the future direction of the time series changes depends on the previous weighted changes.  Double exponential smoothing, a.k.a. the Holt method is defined by the relationship:\n",
        "\n",
        "\\begin{equation}\n",
        "S_t = \\alpha X_t + (1 - \\alpha) (S_{t-1} + b_{t-1})\\\\\n",
        "b_t = \\beta (S_t - S_{t-1}) + (1- \\beta) b_{t-1}\n",
        "\\end{equation}\n",
        "\n",
        "where $S_1 = X_1$, $\\quad b_1 = X_1 - X_0$ and $\\alpha, \\beta \\in (0,1)$\n",
        "\n",
        "The first equation describes the intercept, which depends on the current value of the series. The second term is now split into previous values of the level and of the trend. The second function describes the trend, which depends on the level changes at the current step and on the previous value of the trend. Complete prediction is composed of the sum of level and trend and the difference with simple exponential smoothing is that we need a second parameter to smooth the trend - as before, those can be set based on expert judgement or estimated (jointly) from the data.\n",
        "\n",
        "The forecast $h$ steps ahead is defined by\n",
        "\\begin{equation}\n",
        "\\hat{X}_{t+h} = S_t + h b_t\n",
        "\\end{equation}\n",
        "\n",
        "The forecast function is no longer flat but trending: $h$-step-ahead forecast is equal to the last estimated level plus $h$ times the last estimated trend value."
      ],
      "metadata": {
        "id": "RJTdGWUtfZIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the performance of the two methods on the passenger dataset:"
      ],
      "metadata": {
        "id": "Dwfrf1QDfZIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "alpha = 0.5\n",
        "df = pd.read_csv(CFG.data_folder + 'passengers.csv', usecols = ['passengers'])\n",
        "df.plot.line()\n",
        "fit1 = SimpleExpSmoothing(df).fit(smoothing_level= alpha,optimized=False)\n",
        "fcast1 = fit1.forecast(12).rename(r'$\\alpha=0.5$')\n",
        "fcast1.plot(marker='o', color='red', legend=True)\n",
        "fit1.fittedvalues.plot(  color='red')\n",
        "plt.show()\n",
        "\n",
        "df.plot.line()\n",
        "\n",
        "fit1 = Holt(df).fit(smoothing_level=0.5, smoothing_slope=0.5, optimized=False)\n",
        "fcast1 = fit1.forecast(12).rename(\"Holt's linear trend\")\n",
        "fit1.fittedvalues.plot(color='red')\n",
        "fcast1.plot(color='red', legend=True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:14.143664Z",
          "iopub.execute_input": "2021-07-15T22:38:14.144062Z",
          "iopub.status.idle": "2021-07-15T22:38:14.718794Z",
          "shell.execute_reply.started": "2021-07-15T22:38:14.144022Z",
          "shell.execute_reply": "2021-07-15T22:38:14.717739Z"
        },
        "trusted": true,
        "id": "sEPutHwWfZIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like we are moving in the right direction - the forecast going forward is not constant, but follows a trend. However, it is simply an extrapolation of the most recent (smoothed) trend in the data which means we can expect the forecast to turn negative shortly. This is suspicious in general, and clearly renders the forecast unusable in the domain context."
      ],
      "metadata": {
        "id": "Kp0ZbtR_fZIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Triple Exponential Smoothing\n",
        "\n",
        "If it worked once, maybe it can work twice? A natural extension is to introduce a smoothed seasonal component: triple exponential smoothing, a.k.a. Holt-Winters, is defined by:\n",
        "\n",
        "\\begin{equation}\n",
        "S_t = \\alpha (X_t - c_{t - L}) + (1 - \\alpha) (S_{t-1} + b_{t-1}) \\\\\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "b_t = \\beta (S_t - S_{t-1}) + (1- \\beta) b_{t-1} \\\\\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "c_t = \\gamma (X_t - S_{t-1} - b_{t-1})+ (1 - \\gamma) c_{t-L}\n",
        "\\end{equation}\n",
        "\n",
        "with $\\alpha, \\beta, \\gamma \\in (0,1)$.\n",
        "\n",
        "The most important addition is the seasonal component to explain repeated variations around intercept and trend, and it will be specified by the period. For each observation in the season, there is a separate component; for example, if the length of the season is 7 days (a weekly seasonality), we will have 7 seasonal components, one for each day of the week. An obvious, yet worth repeating caveat: it makes sense to estimate seasonality with period $L$ only if your sample size is bigger than $2L$.\n",
        "\n",
        "The forecast $h$ steps ahead is defined by\n",
        "\\begin{equation}\n",
        "\\hat{X}_{t+h} = S_t + h b_t + c_{(t-L + h) \\;\\; mod \\;\\;  L }\n",
        "\\end{equation}\n",
        "\n"
      ],
      "metadata": {
        "id": "hxUDrjYpfZIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.5\n",
        "df = pd.read_csv(CFG.data_folder + 'passengers.csv', usecols = ['passengers'])\n",
        "df.plot.line()\n",
        "fit1 = ExponentialSmoothing(df, seasonal_periods=12, trend='add', seasonal='add')\n",
        "fit1 = fit1.fit(smoothing_level=0.5,use_boxcox=True)\n",
        "fit1.fittedvalues.plot(color='red')\n",
        "fit1.forecast(12).rename(\"Holt-Winters smoothing\").plot(color='red', legend=True)\n",
        "\n",
        "plt.ylim(0, 800); plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:14.720145Z",
          "iopub.execute_input": "2021-07-15T22:38:14.720453Z",
          "iopub.status.idle": "2021-07-15T22:38:15.04765Z",
          "shell.execute_reply.started": "2021-07-15T22:38:14.720422Z",
          "shell.execute_reply": "2021-07-15T22:38:15.046692Z"
        },
        "trusted": true,
        "id": "BjmQOpcjfZIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, incorporating both the trend and seasonality explicitly leads to a much higher quality forecast."
      ],
      "metadata": {
        "id": "LJShFMmNfZIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taxonomy\n",
        "\n",
        "\n",
        "We conclude that part by discussing briefly the taxonomy of smoothing models following \"Forecasting: Principles and Practice\"\n",
        "by Hyndman and Athanasopoulos, chapter 7 (where the table below is taken from): the ones discussed above are the most popular ones, but not the only ones possible. By considering variations in the combinations of the trend and seasonal components, nine exponential smoothing methods are possible: each method is labelled by a pair of letters (T,S) defining the type of ‘Trend’ and ‘Seasonal’ components. For example, (A,M) is the method with an additive trend and multiplicative seasonality; (Ad ,N) is the method with damped trend and no seasonality; and so on.\n",
        "\n",
        "![expo_smoo_table.png](attachment:e56a2ba9-f663-45fe-99ba-5b088b087630.png)\n",
        "\n",
        "Some of these methods we have already seen using other names:\n",
        "* (N,N) is simple exponential smoothing\n",
        "* (A,N) is Holt's linear trend method\n",
        "* (A,A) corresponds to additive Holt-Winters method\n",
        "\n",
        "If you have some time on your hands, implementing some other methods might be a good exercise - but the word \"practical\" is there for a reason in the title, so we focus on using tools available out of the box - and understanding what we're doing."
      ],
      "metadata": {
        "id": "0fJZKDwXfZIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section-three\"></a>\n",
        "# Anomaly detection"
      ],
      "metadata": {
        "id": "SJTDrvmNfZIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection is a process for identifying unexpected data, event or behavior that require some examination. It is a well-established field within data science and there is a large number of algorithms to detect anomalies in a dataset depending on data type and business context. One of the simplest, yet surprisingly effective ones, is a Z-score. The basic idea is that after suitable normalization, most kinds of data starts behaving like a Gaussian distribution (look up Central Limit Theorem if you would like to understand why) - which implies we only need mean and standard deviation to assess if a given observation is an outlier. Keep in mind that this approach is not 100pct foolproof: there are practical situations when CLT does not work (there's a reason it has a list of assumptions :-) and the Z score will not be meaningful in such circumstances; it does not happen very often, but it is not impossible.\n",
        "\n",
        "\n",
        "The calculation of the Z-score for each individual point in the series proceeds in the following manner:\n",
        "* select a window size $w$\n",
        "* calculate rolling mean / standard deviation with window $w$\n",
        "* demean and normalize by sd:\n",
        "\\begin{equation}\n",
        "Z_t = \\left|\\frac{X_t - \\bar{X}_m}{\\sigma_m} \\right|\n",
        "\\end{equation}\n",
        "\n",
        "* Z-score measures number of sd away from mean $\\implies$ values above 3 indicate extremely unlikely realization"
      ],
      "metadata": {
        "id": "oCe-mBRcfZIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load\n",
        "series = pd.read_csv(CFG.data_folder + 'ambient_temperature_system_failure.csv')\n",
        "series['timestamp'] = pd.to_datetime(series['timestamp'])\n",
        "\n",
        "# plot\n",
        "series['value'].plot()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:15.048984Z",
          "iopub.execute_input": "2021-07-15T22:38:15.049293Z",
          "iopub.status.idle": "2021-07-15T22:38:15.284729Z",
          "shell.execute_reply.started": "2021-07-15T22:38:15.049263Z",
          "shell.execute_reply": "2021-07-15T22:38:15.283957Z"
        },
        "trusted": true,
        "id": "hBBeynvqfZIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick a window size\n",
        "window_size = 25\n",
        "\n",
        "# calculate rolling mean and standard deviation\n",
        "xroll = series['value'].rolling(window_size)\n",
        "series['mean_roll'] = xroll.mean()\n",
        "series['sd_roll'] = xroll.std()\n",
        "\n",
        "# calculate the Z-score\n",
        "series['zscore'] = np.abs( (series['value'] - series['mean_roll']) / series['sd_roll'])\n",
        "series['zscore'].plot()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:15.285796Z",
          "iopub.execute_input": "2021-07-15T22:38:15.286176Z",
          "iopub.status.idle": "2021-07-15T22:38:15.551528Z",
          "shell.execute_reply.started": "2021-07-15T22:38:15.286146Z",
          "shell.execute_reply": "2021-07-15T22:38:15.55045Z"
        },
        "trusted": true,
        "id": "-IIFcJHbfZIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check which observations are out of range\n",
        "series.loc[series['zscore'] > 3][['timestamp', 'value']]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:15.553955Z",
          "iopub.execute_input": "2021-07-15T22:38:15.554349Z",
          "iopub.status.idle": "2021-07-15T22:38:15.572217Z",
          "shell.execute_reply.started": "2021-07-15T22:38:15.554314Z",
          "shell.execute_reply": "2021-07-15T22:38:15.571047Z"
        },
        "trusted": true,
        "id": "2KaUZUcofZIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all together\n",
        "\n",
        "Now that we have all the pieces prepared, we can combine them to build a predictive model. We will use the dataset on daily US energy consumption (in billion kWh)"
      ],
      "metadata": {
        "id": "S2R9aUrOfZIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(CFG.data_folder + 'us_energy.csv')\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.set_index('date', inplace = True)\n",
        "df.plot()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:15.573963Z",
          "iopub.execute_input": "2021-07-15T22:38:15.57427Z",
          "iopub.status.idle": "2021-07-15T22:38:15.842663Z",
          "shell.execute_reply.started": "2021-07-15T22:38:15.574241Z",
          "shell.execute_reply": "2021-07-15T22:38:15.841593Z"
        },
        "trusted": true,
        "id": "jg5_yepyfZId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with a seasonal decomposition (introduced in [part 1](https://www.kaggle.com/konradb/practical-time-series-part-1-the-basics)) to get a basic understanding of the dynamics:"
      ],
      "metadata": {
        "id": "SrBvQTMtfZId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perform seasonal decomposition\n",
        "decomposition = sm.tsa.seasonal_decompose(df,period =12)\n",
        "figure = decomposition.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:15.844228Z",
          "iopub.execute_input": "2021-07-15T22:38:15.844709Z",
          "iopub.status.idle": "2021-07-15T22:38:16.730522Z",
          "shell.execute_reply.started": "2021-07-15T22:38:15.844661Z",
          "shell.execute_reply": "2021-07-15T22:38:16.729562Z"
        },
        "trusted": true,
        "id": "U_CMKuyJfZId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decomposition = sm.tsa.seasonal_decompose(df[\"value\"],period =12, model = 'multiplicative')\n",
        "figure = decomposition.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:16.731824Z",
          "iopub.execute_input": "2021-07-15T22:38:16.732129Z",
          "iopub.status.idle": "2021-07-15T22:38:17.619623Z",
          "shell.execute_reply.started": "2021-07-15T22:38:16.732098Z",
          "shell.execute_reply": "2021-07-15T22:38:17.61846Z"
        },
        "trusted": true,
        "id": "DuTTzK7dfZId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of the two graphs above shows a few things:\n",
        "* there is definitely a trend in the data\n",
        "* we have a clear seasonal pattern\n",
        "* more stable behavior of the residuals in the second case suggests that a multiplicative decomposition is more appropriate.\n",
        "\n",
        "Next step: we split the data into training and validation. Our cutoff point will be 2005:"
      ],
      "metadata": {
        "id": "JpRDyJAofZIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cutoff_date = '2005-12-31'\n",
        "xtrain, xvalid  = df.loc[df.index <= cutoff_date], df.loc[df.index > cutoff_date]\n",
        "print(xtrain.shape, xvalid.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:17.621276Z",
          "iopub.execute_input": "2021-07-15T22:38:17.62199Z",
          "iopub.status.idle": "2021-07-15T22:38:17.63045Z",
          "shell.execute_reply.started": "2021-07-15T22:38:17.621944Z",
          "shell.execute_reply": "2021-07-15T22:38:17.629566Z"
        },
        "trusted": true,
        "id": "2CmNUTB7fZIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can fit hte model now:"
      ],
      "metadata": {
        "id": "BXECAbblfZIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fit1 = ExponentialSmoothing(xtrain['value'].values, seasonal_periods=12, trend='mul', seasonal='mul')\n",
        "fit1 = fit1.fit(use_boxcox=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:17.631755Z",
          "iopub.execute_input": "2021-07-15T22:38:17.632248Z",
          "iopub.status.idle": "2021-07-15T22:38:17.982992Z",
          "shell.execute_reply.started": "2021-07-15T22:38:17.632195Z",
          "shell.execute_reply": "2021-07-15T22:38:17.982129Z"
        },
        "trusted": true,
        "id": "Fa2ALm3YfZIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examining the parameters is fairly straightforward - as you can see, we did not specify the values of the smoothing constants, so they were automatically estimated."
      ],
      "metadata": {
        "id": "dEbSU0UzfZIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fit1.params_formatted"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:38:17.984302Z",
          "iopub.execute_input": "2021-07-15T22:38:17.984654Z",
          "iopub.status.idle": "2021-07-15T22:38:17.999222Z",
          "shell.execute_reply.started": "2021-07-15T22:38:17.984619Z",
          "shell.execute_reply": "2021-07-15T22:38:17.998138Z"
        },
        "trusted": true,
        "id": "hT4cs7pRfZIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What do the residuals look like?\n",
        "prediction = fit1.forecast(len(xvalid)).copy()\n",
        "\n",
        "xresiduals = xvalid['value'] - prediction\n",
        "plot_acf(xresiduals, lags = 25)\n",
        "print()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:40:51.508033Z",
          "iopub.execute_input": "2021-07-15T22:40:51.508401Z",
          "iopub.status.idle": "2021-07-15T22:40:51.950038Z",
          "shell.execute_reply.started": "2021-07-15T22:40:51.50837Z",
          "shell.execute_reply": "2021-07-15T22:40:51.948964Z"
        },
        "trusted": true,
        "id": "q0BfrcJzfZIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pacf(xresiduals, lags = 25)\n",
        "print()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:41:11.863484Z",
          "iopub.execute_input": "2021-07-15T22:41:11.863901Z",
          "iopub.status.idle": "2021-07-15T22:41:12.125707Z",
          "shell.execute_reply.started": "2021-07-15T22:41:11.863868Z",
          "shell.execute_reply": "2021-07-15T22:41:12.124645Z"
        },
        "trusted": true,
        "id": "Q3AgL-FufZIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The behavior of ACF / PACF (statistically significant autocorrelations) suggests that there is some first- and second-order dependence that our Holt-Winter model cannot capture."
      ],
      "metadata": {
        "id": "aowWp4bIfZIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "xvalid['prediction'] = prediction\n",
        "xvalid.plot()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-15T22:42:33.142152Z",
          "iopub.execute_input": "2021-07-15T22:42:33.142586Z",
          "iopub.status.idle": "2021-07-15T22:42:33.42326Z",
          "shell.execute_reply.started": "2021-07-15T22:42:33.142536Z",
          "shell.execute_reply": "2021-07-15T22:42:33.42249Z"
        },
        "trusted": true,
        "id": "Ppicp-_HfZIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the graph above, the model is doing a decent job for the first few years in the sample, but starts to overestimate the consumption afterwards - indicating perhaps a change in the nature of the trend (which would be consistent with the ACF pattern above). This confirms the intuition that there are aspects of the data generating process that are not adequately captured by our three prameter model - but for something that simple, you can make a solid case it is acceptable.\n"
      ],
      "metadata": {
        "id": "kSIQBVahfZIh"
      }
    }
  ]
}